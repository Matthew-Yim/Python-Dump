{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3] # 3 node input layer\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3, 2.5] # 4 node hidden layer\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[2]*weights[2] + bias\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "696f63b699652eb3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8, 0.1200000000000001, 0.72]\n"
     ]
    }
   ],
   "source": [
    "# 3 neurons 4 inputs\n",
    "inputs = [1, 2, 3, 2.5] # 4 node hidden layer\n",
    "\n",
    "weights1 = [0.2, 0.8, -0.5, 1.0]\n",
    "weights2 = [0.5, -0.91, -0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "output = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[2]*weights1[2] + bias1,\n",
    "          inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[2]*weights2[2] + bias2,\n",
    "          inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[2]*weights3[2] + bias3]\n",
    "print(output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T16:09:31.652611900Z",
     "start_time": "2024-03-01T16:09:31.584610900Z"
    }
   },
   "id": "b63f38dd0564f03d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3, 2.5] # 4 node hidden layer\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, -0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_outputs = [] # Output of current layer\n",
    "for neuron_weights, neuron_bias in zip(weights, biases): \n",
    "    # zip combines lists into a singular list that has list as elements\n",
    "    neuron_output = 0 # Output of given neuron\n",
    "    for neuron_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += neuron_input*weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "        \n",
    "print(layer_outputs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f70d91b7b2f2b8c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "output = np.dot(weights, inputs) + bias\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T17:59:39.366475400Z",
     "start_time": "2024-03-01T17:59:39.309899700Z"
    }
   },
   "id": "12332f81b2d7b3c9",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.8256  4.21425 4.69065]\n",
      " [4.707   1.6828  8.0213 ]\n",
      " [2.60406 2.9477  3.35479]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix Multiplication is just a dot product, but for matrices\n",
    "\"\"\"Keep in mind that with Matrix multiplication you multiply row by column, and below you can see we have two 3x4   matrices so this will result in a error. \n",
    "    Need to transpose: In order to transpose we have to convert the lol or list of lists into numpy arrays\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "weights2 = [[0.1, -0.14, 0.5],\n",
    "            [-0.5, 0.12, -0.33],\n",
    "            [-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases2 = [-1, 2, -0.7]\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2)) + bias2\n",
    "print(layer2_outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T18:57:43.440711500Z",
     "start_time": "2024-03-01T18:57:43.392190500Z"
    }
   },
   "id": "f59d2831c3ccd7fc",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The way that we initialize weights for a new neural network is to initialize them with random values between \n",
    "            the range of negative 1 and positive 1\n",
    "    Tighter the range the better, so use small values, the reason why we want small values is in hopes that things      continue to trend in the range of negative 1 and positive 1.\n",
    "    Usually with bigger values, as the neural network trains and gradients flow backward though the network during      backpropagation. If the weights are too large, the gradients can explode (become extremely large) or         vanish (become extremely small)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    1. Normalize Dataset: Basically use statistics and box-plot and min-max | stuff like that to remove outliers and to ensure the data being feed into the neural network is clean and without bias or skewing and represent the data as a whole --> kinda like the average of the data\n",
    "    2. Scale the Dataset: So what this means is that some values need to be change in a way that all the values are between negative 1 and positive 1, but still maintain there meaning or true value in reference to the rest of the values in its particular category/column\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    Weights: Tend to initialize them between the values of -0.1 to 0.1\n",
    "    Bias: Tend to initialize them to 0, but not always try a small non-zero number\n",
    "        Example: Let's say that for whatever reason your neurons aren't firing initially so if they go through the neural network, input multiplied by the weights, its not big enough to actually produce an output. With your bias being zero means that neuron is going to output a zero and zero multiplied by anything outputs a zero, --> so you just propageted though the entire network with all zeros\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ec0d1e71e9f4bbf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# Shape: [row, column]\n",
    "X = [[1, 2, 3, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = 0.1 * np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        # could either be the training data (input layer) or (output from hidden layer to next layer)\n",
    "    \n",
    "    \n",
    "layer1 = Layer_Dense(4, 5) #\n",
    "layer2 = Layer_Dense(5, 2)\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:19:42.177623100Z",
     "start_time": "2024-03-04T14:19:42.154580500Z"
    }
   },
   "id": "698513c7a032786b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "\"\"\"\n",
    "    If you use just weights and biases, you would be left with a activation function resembling a linear line of y = x, whatever the input is the output\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T14:30:41.863503300Z",
     "start_time": "2024-03-04T14:30:41.791381900Z"
    }
   },
   "id": "6b38180ea5f81ffc",
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
