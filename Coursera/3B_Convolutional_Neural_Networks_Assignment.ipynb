{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3858d2a0689cdd88"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-01T17:51:23.620462900Z",
     "start_time": "2024-04-01T17:51:09.300569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To figure out size of matrix: [(W−K+2P)/S]+1 = Z --> [Features|Channels, Z, Z]\n",
    "            # W input size | K kernel size | P padding | S Stride\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) # P1:[32, 28, 28] P2:[32, 30, 30]\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1) #P1:[32, 28, 28] P2:[32, 32, 32]\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2) # Stride=kernel_size (loss of data occurs when stride>kernelsize\n",
    "            #P1:[32, 14, 14] P2:[32, 16, 16]\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1) #P1:[64, 14, 14] P2:[64, 18, 18]\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1) #P1:[64, 14, 14] P2:[64, 20, 20]\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2) # pool_kernelSize determines how spatial dimensions are downsampled or reduced in size (kernal_size=2 from 28x28 image = 14x14) since pooling occured twice=7x7\n",
    "            #P1:[64, 7, 7] P2:[64, 10, 10]\n",
    "        \n",
    "        # Flatten\n",
    "        self.fc1 = nn.Linear(7*7*64, 256) # spatial dimension x feature maps or num of channels in output tensor after the convolutional layers\n",
    "        self.fc2 = nn.Linear(256, 10) # Flattening is necessary cuz fully connected layers expect a 1D vector of input features\n",
    "            # Flatten: Why 2 fully connected layers are necessary: If single connected layer, the model would essentially be a linear classifier on top of the convolutional features, which would severely limit the model's ability to learn complex patterns and decision boundaries. By introducing an intermediate fully connected layer, we alow the model to learn a non-linear transformation of the flattened convolutional features, which can capture more complex relationships and patterns in the data.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "       \n",
    "        # conv layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # max pool 1\n",
    "            # x = F.max_pool2d(x, kernel_size=2) same as below\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # conv layer 3\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # conv layer 4\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # max pool 1\n",
    "            # x = F.max_pool2d(x, kernel_size=2) same as below\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 7*7*64)\n",
    "        \n",
    "        # fc layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # fc layer 2\n",
    "        x = self.fc2(x)\n",
    "        return x       "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T17:51:46.116361800Z",
     "start_time": "2024-04-01T17:51:46.095848300Z"
    }
   },
   "id": "b8e7f1f70987bce6",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To figure out size of matrix: [(W−K+2P)/S]+1 = Z --> [Features|Channels, Z, Z] ** Convolution layer\n",
    "            # W input size | K kernel size | P padding | S Stride\n",
    "        \n",
    "        # To figure out size of matrix: [(W−K+2P)/S] = Z --> [Features|Channels, Z, Z] ** Pool layer\n",
    "            # W input size | K kernel size | P padding | S Stride\n",
    "        \n",
    "        # Total parameters = Sum(Convolutional layer) --> [K*K*(previousLayer_num_filters) + stride]*num_filters\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=2) # P1:[32, 28, 28] P2:[32, 30, 30]\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=2) #P1:[32, 28, 28] P2:[32, 32, 32]\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2) # Stride=kernel_size (loss of data occurs when stride>kernelsize\n",
    "            #P1:[32, 14, 14] P2:[32, 16, 16]\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=2) #P1:[64, 14, 14] P2:[64, 18, 18]\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=2) #P1:[64, 14, 14] P2:[64, 20, 20]\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2) # pool_kernelSize determines how spatial dimensions are downsampled or reduced in size (kernal_size=2 from 28x28 image = 14x14) since pooling occured twice=7x7\n",
    "            #P1:[64, 7, 7] P2:[64, 10, 10]\n",
    "        \n",
    "        # Flatten\n",
    "        self.fc1 = nn.Linear(10*10*64, 256) # spatial dimension x feature maps or num of channels in output tensor after the convolutional layers\n",
    "        self.fc2 = nn.Linear(256, 10) # Flattening is necessary cuz fully connected layers expect a 1D vector of input features\n",
    "            # Flatten: Why 2 fully connected layers are necessary: If single connected layer, the model would essentially be a linear classifier on top of the convolutional features, which would severely limit the model's ability to learn complex patterns and decision boundaries. By introducing an intermediate fully connected layer, we alow the model to learn a non-linear transformation of the flattened convolutional features, which can capture more complex relationships and patterns in the data.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "       \n",
    "        # conv layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # max pool 1\n",
    "            # x = F.max_pool2d(x, kernel_size=2) same as below\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # conv layer 3\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # conv layer 4\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # max pool 1\n",
    "            # x = F.max_pool2d(x, kernel_size=2) same as below\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 10*10*64)\n",
    "        \n",
    "        # fc layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # fc layer 2\n",
    "        x = self.fc2(x)\n",
    "        return x       "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T17:24:16.804362100Z",
     "start_time": "2024-04-01T17:24:16.792851Z"
    }
   },
   "id": "4d843b97b55413cd",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the data\n",
    "mnist_train = datasets.MNIST(root=\"./datasets\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./datasets\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T17:51:56.648029700Z",
     "start_time": "2024-04-01T17:51:56.586519500Z"
    }
   },
   "id": "9631ac763466244f",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Training\n",
    "# Instantiate model\n",
    "model = MNIST_CNN() # <---- Change to any model you want here we use MNIST_CNN"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T17:51:59.712023800Z",
     "start_time": "2024-04-01T17:51:59.691485100Z"
    }
   },
   "id": "bc79d3b1fb8f0178",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # <-- Change optimizer of lr if you want"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T17:52:01.701332200Z",
     "start_time": "2024-04-01T17:52:01.677814400Z"
    }
   },
   "id": "59f65acf63e5c2e0",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39764ae904344d1590658b68c8e5cf8f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/600 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc6255b759784929b289863f6ce265eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/600 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "711cda3a821f422a9b311ca007153ba1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/600 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fba7cd781ea4f04ad0dd9bd108c24fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iterate through train set minibatchs\n",
    "for epoch in trange(3): # <--- Change here\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Pass\n",
    "        x = images # <--- Change here\n",
    "        y = model(x)\n",
    "        loss = criterion(y, labels)\n",
    "        \n",
    "        # Backward Pass\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T17:55:19.494758300Z",
     "start_time": "2024-04-01T17:52:04.247230600Z"
    }
   },
   "id": "921b89a1ba24750f",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a638c0734044423c883fbacd5951a79e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9905999898910522\n"
     ]
    }
   ],
   "source": [
    "## Testing \n",
    "correct = 0\n",
    "total = len(mnist_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate through test set minibatchs\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        # Forward Pass\n",
    "        x = images # <--- Change here\n",
    "        y = model(x)\n",
    "        \n",
    "        predictions = torch.argmax(y, dim=1)\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "        \n",
    "print('Test accuracy: {}'.format(correct / total))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T17:56:09.550902200Z",
     "start_time": "2024-04-01T17:56:03.562133500Z"
    }
   },
   "id": "69061bcd97039b0b",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870634\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T17:57:08.999185300Z",
     "start_time": "2024-04-01T17:57:08.968181800Z"
    }
   },
   "id": "2a5ee7fe27f24b98",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 0.989300012588501 #Padding of 1\n",
    "# 0.9882000088691711 #Padding of 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f83792999e9c8d9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
