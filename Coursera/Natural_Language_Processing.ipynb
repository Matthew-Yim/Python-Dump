{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:08:11.046436100Z",
     "start_time": "2024-04-02T20:07:19.005507500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Conceptnet Numberbatch word embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Download word vectors\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "if not os.path.isfile('datasets/mini.h5'):\n",
    "    print(\"Downloading Conceptnet Numberbatch word embeddings...\")\n",
    "    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n",
    "    urlretrieve(conceptnet_url, 'datasets/mini.h5')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words dimensions: 362891\n",
      "all_embeddings dimensions: (362891, 300)\n",
      "Random example word: /c/de/aufmachung\n"
     ]
    }
   ],
   "source": [
    "# Load the file and pull out words and embeddings\n",
    "import h5py\n",
    "\n",
    "with h5py.File('datasets/mini.h5', 'r') as f:\n",
    "    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
    "    all_embeddings = f['mat']['block0_values'][:]\n",
    "    \n",
    "print(\"all_words dimensions: {}\".format(len(all_words)))\n",
    "print(\"all_embeddings dimensions: {}\".format(all_embeddings.shape))\n",
    "\n",
    "print(\"Random example word: {}\".format(all_words[1337]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:08:34.771886500Z",
     "start_time": "2024-04-02T20:08:28.889468600Z"
    }
   },
   "id": "7cdcb89fbba9a8fd",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English words in all_words: 150875\n",
      "english_embeddings dimensions: (150875, 300)\n",
      "activated_carbon\n"
     ]
    }
   ],
   "source": [
    "# Restrict our vocabulary to just the English words\n",
    "english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
    "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
    "english_embeddings = all_embeddings[english_word_indices]\n",
    "\n",
    "print(\"Number of English words in all_words: {0}\".format(len(english_words)))\n",
    "print(\"english_embeddings dimensions: {0}\".format(english_embeddings.shape))\n",
    "\n",
    "print(english_words[1337])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:08:38.802538200Z",
     "start_time": "2024-04-02T20:08:38.677018200Z"
    }
   },
   "id": "188b360e4209dfff",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "norms = np.linalg.norm(english_embeddings, axis=1)\n",
    "normalized_embeddings = english_embeddings.astype('float32') / norms.astype('float32').reshape([-1, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:08:42.449626100Z",
     "start_time": "2024-04-02T20:08:41.347969100Z"
    }
   },
   "id": "24a397064a0972b7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "index = {word: i for i, word in enumerate(english_words)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:08:44.082857200Z",
     "start_time": "2024-04-02T20:08:43.893835100Z"
    }
   },
   "id": "48f024799bd49c31",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\tcat\t 1.0000001\n",
      "cat\tfeline\t 0.8199548\n",
      "cat\tdog\t 0.590724\n",
      "cat\tmoo\t 0.0039538303\n",
      "cat\tfreeze\t -0.030225191\n",
      "antonym\topposite\t 0.3941065\n",
      "antonym\tsynonym\t 0.46883982\n"
     ]
    }
   ],
   "source": [
    "def similarity_score(w1, w2):\n",
    "    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])\n",
    "    return score\n",
    "\n",
    "# A word is as similar with itself as possible:\n",
    "print('cat\\tcat\\t', similarity_score('cat', 'cat'))\n",
    "\n",
    "# Closely related words still get high scores:\n",
    "print('cat\\tfeline\\t', similarity_score('cat', 'feline'))\n",
    "print('cat\\tdog\\t', similarity_score('cat', 'dog'))\n",
    "\n",
    "# Unrelated words, not so much\n",
    "print('cat\\tmoo\\t', similarity_score('cat', 'moo'))\n",
    "print('cat\\tfreeze\\t', similarity_score('cat', 'freeze'))\n",
    "\n",
    "# Antonyms are still considered related, sometimes more so than synonyms\n",
    "print('antonym\\topposite\\t', similarity_score('antonym', 'opposite'))\n",
    "print('antonym\\tsynonym\\t', similarity_score('antonym', 'synonym'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:08:45.405551Z",
     "start_time": "2024-04-02T20:08:45.398551300Z"
    }
   },
   "id": "b33e0d145fcce659",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def closet_to_vector(v, n):\n",
    "    all_scores = np.dot(normalized_embeddings, v)\n",
    "    best_words = list(map(lambda i: english_words[i], reversed(np.argsort(all_scores))))\n",
    "    return best_words[:n]\n",
    "\n",
    "def most_similar(w, n):\n",
    "    return closet_to_vector(normalized_embeddings[index[w], :], n)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:08:47.856427700Z",
     "start_time": "2024-04-02T20:08:47.841429700Z"
    }
   },
   "id": "162b2a9200fbf784",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'humane_society', 'kitten', 'feline', 'colocolo', 'cats', 'kitty', 'maine_coon', 'housecat', 'sharp_teeth']\n",
      "['dog', 'dogs', 'wire_haired_dachshund', 'doggy_paddle', 'lhasa_apso', 'good_friend', 'puppy_dog', 'bichon_frise', 'woof_woof', 'golden_retrievers']\n",
      "['duke', 'dukes', 'duchess', 'duchesses', 'ducal', 'dukedom', 'duchy', 'voivode', 'princes', 'prince']\n"
     ]
    }
   ],
   "source": [
    "print(most_similar('cat', 10))\n",
    "print(most_similar('dog', 10))\n",
    "print(most_similar('duke', 10))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:08:49.749271700Z",
     "start_time": "2024-04-02T20:08:49.560218400Z"
    }
   },
   "id": "29d06e615f0b2a88",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sister']\n",
      "['wife']\n",
      "['paris']\n"
     ]
    }
   ],
   "source": [
    "def solve_analogy(a1, b1, a2):\n",
    "    b2 = normalized_embeddings[index[b1], :] - normalized_embeddings[index[a1], :] + normalized_embeddings[index[a2], :]\n",
    "    return closet_to_vector(b2, 1)\n",
    "\n",
    "print(solve_analogy(\"man\", \"brother\", \"woman\"))\n",
    "print(solve_analogy(\"man\", \"husband\", \"woman\"))\n",
    "print(solve_analogy(\"spain\", \"madrid\", \"france\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:08:51.439023200Z",
     "start_time": "2024-04-02T20:08:51.239991Z"
    }
   },
   "id": "37d701dc442b9fa6",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using word embeddings in Deep Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cd6d2488004936f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/movie-simple.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 22\u001B[0m\n\u001B[0;32m     20\u001B[0m xs \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     21\u001B[0m ys \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdatasets/movie-simple.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mignore\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m f\u001B[38;5;241m.\u001B[39mreadlines():\n\u001B[0;32m     24\u001B[0m         x, y \u001B[38;5;241m=\u001B[39m convert_line_to_example(l)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Coursera_Duke_ML\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    305\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    308\u001B[0m     )\n\u001B[1;32m--> 310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'datasets/movie-simple.txt'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "remove_punct = str.maketrans('','',string.punctuation)\n",
    "# This function converts a line of our data file into a tuple (x,y), where x is 300-dimensional representation of the words in a review, and y is its label\n",
    "\n",
    "def convert_line_to_example(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "    \n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    \n",
    "    # Look up the embedding of each word, ignoring words not in our pre-trained vocabulary\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words if w in index]\n",
    "    \n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    \n",
    "# Apply the function to each line in the file\n",
    "xs = []\n",
    "ys = []\n",
    "with open(\"datasets/movie-simple.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    for l in f.readlines():\n",
    "        x, y = convert_line_to_example(l)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        \n",
    "# Concatenate all examples into a numpy array\n",
    "xs = np.vstack(xs)\n",
    "ys = np.vstack(ys)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:08:57.961536900Z",
     "start_time": "2024-04-02T20:08:55.498659700Z"
    }
   },
   "id": "b26a6708653e3064",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"Shape of inputs: {}\".format(xs.shape))\n",
    "print(\"Shape of labels: {}\".format(ys.shape))\n",
    "\n",
    "num_examples = xs.shape[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2902ea2743c2c6a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"First 20 labels before shuffling: {0}\".format(ys[:20,0]))\n",
    "\n",
    "shuffle_idx = np.random.permutation(num_examples)\n",
    "xs = xs[shuffle_idx, :]\n",
    "ys = ys[shuffle_idx, :]\n",
    "\n",
    "print(\"First 20 labels after shuffling: {0}\".format(ys[:20, 0]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8edd0a45e37f6a7a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "num_train = 4*num_examples // 5\n",
    "\n",
    "x_train = torch.tensor(xs[:num_train])\n",
    "y_train = torch.tensor(ys[:num_train], dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor(xs[num_train:])\n",
    "y_test = torch.tensor(ys[num_train:], dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf7e300fc791512b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reviews_train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "reviews_test = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(reviews_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(reviews_test, batch_size=100, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "512b00e482effb0c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb178cf39ded1e68"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SWEM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "945ebb08bdf8b4b3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Training\n",
    "# Instantiate model\n",
    "\n",
    "model = SWEM()\n",
    "\n",
    "# Binary cross-entropy (BCE) Loss and Adam Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Iterate through train set minibatchs\n",
    "for epoch in range(250):\n",
    "    correct = 0\n",
    "    num_examples = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        y = model(inputs)\n",
    "        loss = criterion(y, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = torch.round(torch.sigmoid(y))\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "        num_examples += len(inputs)\n",
    "        \n",
    "    # Print training progress\n",
    "    if epoch % 25 == 0:\n",
    "        acc = correct/num_examples\n",
    "        print(\"Epoch: {0} \\t Train Loss: {1} \\t Train Acc: {2}\".format(epoch, loss, acc))\n",
    "        \n",
    "## Testing \n",
    "correct = 0\n",
    "num_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate through test set minibatchs\n",
    "    for inputs, labels in test_loader:\n",
    "        # Forward pass\n",
    "        y = model(inputs)\n",
    "        \n",
    "        predictions = torch.round(torch.sigmoid(y))\n",
    "        correct ++ torch.sum((predictions == labels).float())\n",
    "        num_test += len(inputs)\n",
    "        \n",
    "print('Test accuracy: {}'.format(correct/num_test))\n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c698b36d849795a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Check some words\n",
    "words_to_test = [\"exciting\", \"hated\", \"boring\", \"loved\"]\n",
    "\n",
    "for word in words_to_test:\n",
    "    x = torch.tensor(normalized_embeddings[index[word]].reshape(1, 300))\n",
    "    print(\"Sentiment of teh word '{0}': {1}\".format(word, torch.sigmoid(model(x))))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8680187f6adab0e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning Word Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a32b483bbd57328a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "EMBED_DIM = 300\n",
    "embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM)\n",
    "\n",
    "embedding.weight.size()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90b10cc32a0f6ffe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SWEMWithEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_dim, num_outputs):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.fc1 = nn.Linear(EMBED_DIM, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=0)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36b9ba7b7eb6443"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = SWEMWithEmbeddings(\n",
    "    vocab_size = 5000,\n",
    "    embedding_size = 300,\n",
    "    hidden_dim = 64,\n",
    "    num_outputs = 1,\n",
    ")\n",
    "print(model)                                                                                                                                                                "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "115a44445c38dfb6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recurrent Neural Networks (RNNs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f00caf55391f5d80"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mb = 1\n",
    "x_dim = 300\n",
    "sentence = [\"recurrent\", \"neural\", \"networks\", \"are\", \"great\"]\n",
    "\n",
    "xs = []\n",
    "for word in sentence:\n",
    "    xs.appen(torch.tensor(normalized_embeddings[index[word]]).view(1, x_dim))\n",
    "    \n",
    "xs = torch.stack(xs, dim=0)\n",
    "print(\"xs.shape: {}\".format(xs.shape))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b5ad7e4a3434ec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# As always, import PyTorch first\n",
    "import numpy as np\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c53bfb7728b92d96"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "h_dim = 128\n",
    "\n",
    "# For projecting the input\n",
    "Wx = torch.randn(x_dim, h_dim)/np.sqrt(x_dim)\n",
    "Wx.requires_grad_()\n",
    "bx = torch.zeros(h_dim, requires_grad=True)\n",
    "\n",
    "# For projecting the previous state\n",
    "Wh = torch.randn(h_dim, h_dim)/np.sqrt(h_dim)\n",
    "Wh.requires_grad_()\n",
    "bh = torch.zeros(h_dim, requires_grad=True)\n",
    "\n",
    "print(Wx.shape, bx.shape, Wh.shape, bh.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c7d37d4873b3e95"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def RNN_step(x, h):\n",
    "    h_next = torch.tanh((torch.matmul(x, Wx) + bx) + (torch.matmul(h, Wh) + bh))\n",
    "    return h_next"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac362ca05883e1ba"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Word embedding for first word\n",
    "x1 = xs[0, :, :]\n",
    "\n",
    "# Initialize hidden state to 0\n",
    "h0 = torch.zeros([mb, h_dim])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe910ef32e9dc493"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Forward pass of one RNN step for time step t=1\n",
    "h1 = RNN_step(x1, h0)\n",
    "\n",
    "print(\"Hidden state h1 dimensions: {0}\".format(h1.shape))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a61d8fb99ad40330"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Word embedding for second word\n",
    "x2 = xs[1, :, :]\n",
    "\n",
    "# Forward pass of one RNN step for time step t=2\n",
    "h2 = RNN_step(x2, h1)\n",
    "\n",
    "print(\"Hidden state h2 dimensions: {0}\".format(h2.shape))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7789316aea16f5c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recurrent Neural Networks via Torch.nn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a0cb67b76539f06"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "rnn = nn.RNN(x_dim, h_dim)\n",
    "print(\"RNN parameter shapes: {}\".format([p.shape for p in rnn.parameters()]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63aa80a40d165019"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hs, h_T = rnn(xs)\n",
    "\n",
    "print(\"Hidden states shape: {}\".format(hs.shape))\n",
    "print(\"Final hidden state shape: {}\".format(h_T.shape))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a5fc444f195a650"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gated Recurrent Neural Networks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fc3dda86232a885"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(x_dim, h_dim)\n",
    "print(\"LSTM parameters: {}\".format([p.shape for p in lstm.parameters()]))\n",
    "\n",
    "gru = nn.GRU(x_dim, h_dim)\n",
    "print(\"GRU parameters: {}\".format([p.shape for p in gru.parameters()]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c951a276f95cd457"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
