{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression Model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8022f8469669f41"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:29:03.857710400Z",
     "start_time": "2024-03-28T13:29:03.812198500Z"
    }
   },
   "id": "3eeb6f02055e6b77",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch AutoGrad Mechanics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8747d1c7562f5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The optimizer in PyTorch, such as \"torch.optim.SGD\", knows how to compute the gradients of the loss with respect to the model parameters (e.g., weights and biases) through the use of PyTorch's automatic differentiation system, known as Autograd. Here's how it works:\n",
    "    1. Autograd Mechanics: PyTorch's Autograd system automatically computes the gradients of tensors. When you perform operations on tensors that have \"requires_grad=True\", PyTorch keeps track of these operations. When you call \".backward()\" on a tensor (usually the loss tensor), Autograd computes the gradient of that tensor with respect to all tensors that have \"requires_grad=True\". These gradients are stored in teh \".grad\" attribute of each tensor.\n",
    "    2. Gradients Computation: In the provided code snippet, after the forward pass and the computation of the loss (\"cross_entropy\"), calling \"cross_entropy.backward()\" triggers the computation of gradients for all tensors involved in the computation of teh loss that have \"requires_grad=True\". This includes the model parameters \"w\" and \"b\"\n",
    "    3. Optimizer Updates Parameters: Once the gradients are computed, the optimizer (\"torch.optim.SGD\" in this case) uses these gradients to update teh model parameters. The optimizer adjusts the parameters in teh direction that minimizes the loss. This is done by subtracting the gradient times the learning rate from teh current parameter values. The optimizer does not need to know the value of the loss itself; it only needs the gradients to perform the update\n",
    "Summary: The optimizer in PyTorch knows how to compute the gradients because it relies on PyTorch's Autograd system, which automatically computes gradients for tensors involved in operations that have \"requires_grad=True\". The optimizer then uses these gradients to update the model parameters, enabling the learning process."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3aad01f421d0ef5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 20369357.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 28821245.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1648877/1648877 [00:00<00:00, 13241781.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "mnist_train = datasets.MNIST(root=\"./datasets\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./datasets\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:29:14.434153200Z",
     "start_time": "2024-03-28T13:29:11.683188900Z"
    }
   },
   "id": "f9d939b1bb00a342",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training\n",
    "# Initialize parameters\n",
    "# Xavier Initialization, assist in not making gradient vanish/explode \n",
    "# and other benefits compared to making weights initially zeros which is why W = equation below\n",
    "\n",
    "W = torch.randn(784, 10)/np.sqrt(784)\n",
    "W.requires_grad_() # Tells PyTorch autograd to track the gradients \n",
    "b = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD([W,b], lr=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:29:29.038507800Z",
     "start_time": "2024-03-28T13:29:28.995508100Z"
    }
   },
   "id": "a0eaad6339b4150b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/600 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d96fc7e08de4e2985c64770c4cadde2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iterate through train set minibatch\n",
    "for images, labels in tqdm(train_loader):\n",
    "    # Zero out the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    x = images.view(-1, 28*28) # 1D array [1, 784] | the \"-1\" auto calculate the size of the first dimension\n",
    "    y = torch.matmul(x, W) + b # Matrix Multiplication\n",
    "    cross_entropy = F.cross_entropy(y, labels) # F.cross_entropy applies SoftMax as well as the cross-entropy loss\n",
    "        # SoftMax: Normalize probabilities (output of matrixMultiply [y])\n",
    "            # Normalizing here makes probs positive and add up to 1, softmax eq. makes inputs into outputs between 0 and 1 such that the sum of the outputs add up to 1\n",
    "        # Cross-Entropy Loss:\n",
    "    \n",
    "    # Backward Pass\n",
    "    cross_entropy.backward() # calculate gradient (gradients = slope)\n",
    "    optimizer.step() # updates values based on gradient (new_values = old_value - (learningRate * gradient)\n",
    "        # LearningRate (LR): dictates the step size of update\n",
    "        # Gradient: indicates the direction of which the loss decreases\n",
    "        # Keep in mind that LR and gradient are multiplied and with SGD gradient might be big causing a large influence in step or update"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:29:54.211517300Z",
     "start_time": "2024-03-28T13:29:45.235226800Z"
    }
   },
   "id": "cf8a28cf21da3249",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a3e7e9bc3724e83b7614c3d9d0cd2e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9028000235557556\n"
     ]
    }
   ],
   "source": [
    " # Testing\n",
    "correct = 0\n",
    "total = len(mnist_test)\n",
    "\n",
    "# Keep in mind testing occurs after the model is learned and the values of W and b are the latest or perhaps the most optimal\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate through test set minibatchs\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        # Forward pass\n",
    "        x = images.view(-1, 28*28)\n",
    "        y = torch.matmul(x, W) + b\n",
    "        \n",
    "        # Keep in mind that \"y\" is now a tensor/array/1D matrix --> [prob0, prob1, prob2, prob3, prob4, prob5, prob6, prob7, prob8, prob9] and torch.argmax returns the largest prob, which is the prediction\n",
    "        \n",
    "        predictions = torch.argmax(y, dim=1)\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "        \n",
    "print('Test accuracy: {}'.format(correct/total))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:30:46.262223700Z",
     "start_time": "2024-03-28T13:30:44.704525900Z"
    }
   },
   "id": "f53008f58bad5b2e",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extra Notes on Non-Linearity Activation Function:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccad0139971d75bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Non-linearity in the context of neural networks refers to the ability of the network to model complex, non-linear relationships between inputs and outputs. This is crucial for tasks where the underlying data or the relationships between features are not linear. Non-linearity is introduced into neural networks through the use of non-linear activation functions and the architecture of the network itself, particularly through the use of multiple layers. Example: ReLu function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "250305c8ab43db68"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eadd32a325c8d8b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
